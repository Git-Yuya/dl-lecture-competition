# Configuration for training parameters
batch_size: 128      # Number of samples per gradient update
epochs: 50           # Number of complete passes through the training dataset
hid_dim: 1024        # Hidden layer dimension
lr: 0.001            # Learning rate for the optimizer
weight_decay: 0.01   # Weight decay for the optimizer (AdamW)

# Device configuration
device: cuda:0       # Device to be used for computation (e.g., cuda:0 for the first GPU, or cpu for using the CPU)
num_workers: 4       # Number of subprocesses to use for data loading
seed: 1234           # Random seed for reproducibility
use_wandb: False     # Boolean flag to enable or disable Weights and Biases logging

# Data directory
data_dir: data       # Directory where the data is stored

# Evaluation configuration
model_path:          # Path to the model file for evaluation (leave blank if not used)
